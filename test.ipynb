{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_labels = {0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 4: 'sadness', 5: 'surprise', 6: 'neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.load(\"x_test.npy\")\n",
    "y_test = np.load(\"y_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para el perceptrón"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 548us/step\n",
      "Accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo\n",
    "model_perceptron = tf.keras.models.load_model('best_model_perceptron.keras')\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred_prob = model_perceptron.predict(x_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Convertir etiquetas one-hot a formato de clase\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluar la precisión del modelo\n",
    "accuracy = accuracy_score(y_test_classes, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción 1: happiness\n",
      "Clase real 1: neutral\n",
      "---+---\n",
      "Predicción 2: happiness\n",
      "Clase real 2: surprise\n",
      "---+---\n",
      "Predicción 3: happiness\n",
      "Clase real 3: surprise\n",
      "---+---\n",
      "Predicción 4: happiness\n",
      "Clase real 4: fear\n",
      "---+---\n",
      "Predicción 5: happiness\n",
      "Clase real 5: happiness\n",
      "---+---\n"
     ]
    }
   ],
   "source": [
    "# Imprimir algunas predicciones\n",
    "num_predictions = 5\n",
    "for i in range(num_predictions):\n",
    "    pred_label = emotions_labels[y_pred[i]]\n",
    "    random_index = random.randint(0, len(y_pred)-1)\n",
    "    actual_label = emotions_labels[y_test_classes[random_index]]\n",
    "    print(f'Predicción {i+1}: {pred_label}')\n",
    "    print(f'Clase real {i+1}: {actual_label}')\n",
    "    print('---+---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para ELM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class elm(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_neurons=1000, regressor=None):\n",
    "        self.n_hidden_neurons = n_hidden_neurons\n",
    "        self.regressor = regressor\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    \n",
    "    def fit(self, X, y, binary_weights = False):\n",
    "        # Dependiendo de la selección de pesos binarios, se eligen pesos aleatorios o no\n",
    "        input_size = X.shape[1]\n",
    "\n",
    "        if binary_weights:\n",
    "            self.input_weights = np.random.choice([-1, 1], size=(input_size, self.n_hidden_neurons))\n",
    "            self.biases = np.random.choice([-1, 1], size=self.n_hidden_neurons)\n",
    "        else:\n",
    "            self.input_weights = np.random.randn(input_size, self.n_hidden_neurons)\n",
    "            self.biases = np.random.randn(self.n_hidden_neurons)\n",
    "        \n",
    "        # Salida de la capa oculta\n",
    "        H = self._sigmoid(np.dot(X, self.input_weights) + self.biases)\n",
    "        \n",
    "        # Regresión usando regularización si se especifica\n",
    "        self.regressor.fit(H, y)\n",
    "\n",
    "        # Calcular las predicciones sobre el conjunto de entrenamiento\n",
    "        y_pred_train = self.predict(X)\n",
    "        y_true_labels = np.argmax(y, axis=1)\n",
    "        y_pred_labels = np.argmax(y_pred_train, axis=1)\n",
    "        \n",
    "        accuracy_train = accuracy_score(y_true_labels, y_pred_labels)\n",
    "        print(f\"Accuracy en el conjunto de entrenamiento: {accuracy_train:.4f}\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        H = self._sigmoid(np.dot(X, self.input_weights) + self.biases)\n",
    "        return self.regressor.predict(H)\n",
    "    \n",
    "    def save(self, filename):\n",
    "        # pa guardar el modelo\n",
    "        model_data = {\n",
    "            'input_weights': self.input_weights,\n",
    "            'biases': self.biases,\n",
    "            'regressor': self.regressor\n",
    "        }\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "    \n",
    "    def load(self, filename):\n",
    "        # pa cargar el modelo\n",
    "        with open(filename, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        self.input_weights = model_data['input_weights']\n",
    "        self.biases = model_data['biases']\n",
    "        self.regressor = model_data['regressor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_scaled = StandardScaler().fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para EML con pesos no binarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sin regularización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del modelo cargado: 0.2257\n"
     ]
    }
   ],
   "source": [
    "elm_no_reg = elm(n_hidden_neurons=1000, regressor=Ridge(alpha=0.0))\n",
    "elm_no_reg.load('elm_no_reg.keras')\n",
    "\n",
    "y_pred_prob = elm_no_reg.predict(x_test_scaled)\n",
    "acc_loaded = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred_prob, axis=1))\n",
    "print(f\"Accuracy del modelo cargado: {acc_loaded:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sale aún más bajo que el perceptrón.\n",
    "### Con regularización Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del modelo cargado: 0.2213\n"
     ]
    }
   ],
   "source": [
    "elm_ridge = elm(n_hidden_neurons=1000, regressor=Ridge(alpha=0.1))\n",
    "elm_ridge.load('elm_ridge.keras')\n",
    "\n",
    "y_pred_prob = elm_ridge.predict(x_test_scaled)\n",
    "acc_loaded = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred_prob, axis=1))\n",
    "print(f\"Accuracy del modelo cargado: {acc_loaded:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Está saliendo inferior al perceptrón.\n",
    "### Con regularización Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del modelo cargado: 0.2503\n"
     ]
    }
   ],
   "source": [
    "elm_lasso = elm(n_hidden_neurons=1000, regressor=Lasso(alpha=0.1))\n",
    "elm_lasso.load('elm_lasso.keras')\n",
    "\n",
    "y_pred_prob = elm_lasso.predict(x_test_scaled)\n",
    "acc_loaded = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred_prob, axis=1))\n",
    "print(f\"Accuracy del modelo cargado: {acc_loaded:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con Lasso parece tener el mismo desempeño que el perceptrón.\n",
    "### Con regularización Elastic-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del modelo cargado: 0.2503\n"
     ]
    }
   ],
   "source": [
    "elm_elastic = elm(n_hidden_neurons=1000, regressor=ElasticNet(alpha=0.1, l1_ratio=0.5))\n",
    "elm_elastic.load('elm_elastic.keras')\n",
    "\n",
    "y_pred_prob = elm_elastic.predict(x_test_scaled)\n",
    "acc_loaded = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred_prob, axis=1))\n",
    "print(f\"Accuracy del modelo cargado: {acc_loaded:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ya se estaba observando, las últimas dos regularizaciones tienen un efecto similar.\n",
    "## Con pesos binarios.\n",
    "### Sin regularización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del modelo cargado: 0.2233\n"
     ]
    }
   ],
   "source": [
    "elm_no_reg_bin = elm(n_hidden_neurons=1000, regressor=Ridge(alpha=0.0))\n",
    "elm_no_reg_bin.load('elm_no_reg_bin.keras')\n",
    "\n",
    "y_pred_prob = elm_no_reg_bin.predict(x_test_scaled)\n",
    "acc_loaded = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred_prob, axis=1))\n",
    "print(f\"Accuracy del modelo cargado: {acc_loaded:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece tener un rendimento similar a su competencia de pesos continuos.\n",
    "### Con regularización Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del modelo cargado: 0.2233\n"
     ]
    }
   ],
   "source": [
    "elm_ridge_bin = elm(n_hidden_neurons=1000, regressor=Ridge(alpha=0.1))\n",
    "elm_ridge_bin.load('elm_ridge_bin.keras')\n",
    "\n",
    "y_pred_prob = elm_ridge_bin.predict(x_test_scaled)\n",
    "acc_loaded = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred_prob, axis=1))\n",
    "print(f\"Accuracy del modelo cargado: {acc_loaded:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hay un efecto significativo al modelo sin regularización.\n",
    "### Con regularización Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del modelo cargado: 0.2503\n"
     ]
    }
   ],
   "source": [
    "elm_lasso_bin = elm(n_hidden_neurons=1000, regressor=Lasso(alpha=0.1))\n",
    "elm_lasso_bin.load('elm_lasso_bin.keras')\n",
    "\n",
    "y_pred_prob = elm_lasso_bin.predict(x_test_scaled)\n",
    "acc_loaded = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred_prob, axis=1))\n",
    "print(f\"Accuracy del modelo cargado: {acc_loaded:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitivamente, se está comportando muy similar a su competencia de pesos continuos.\n",
    "### Con regularización Elastic-net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del modelo cargado: 0.2503\n"
     ]
    }
   ],
   "source": [
    "elm_elastic_bin = elm(n_hidden_neurons=1000, regressor=ElasticNet(alpha=0.1, l1_ratio=0.5))\n",
    "elm_elastic_bin.load('elm_elastic_bin.keras')\n",
    "\n",
    "y_pred_prob = elm_elastic_bin.predict(x_test_scaled)\n",
    "acc_loaded = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred_prob, axis=1))\n",
    "print(f\"Accuracy del modelo cargado: {acc_loaded:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El comportamiento con los datos de prueba parece no ser muy distinto entre los modelos de ELM, además el término de regularización no aporta demasiado, y en los que lo hace, con dificultades alcanza la precisión del MLP.\n",
    "Aunque ciertamente el MLP requiere de un poco más de poder de cómputo debido al entrenamiento, la realidad es que es mínimo, y ya que está entrenado, sin dificultades alcanza la misma precisión que el modelo de ELM con la regularización más fuerte.\n",
    "\n",
    "Una posible explicación de esto es que, al ser solo los indices correspondientes a los vectores de embeddings, al ser el ajuste correspondiente de la salida del encoder de la VQ-VAE, al estar en un espacio latente más reducido, se puede dar el caso en que dos rostros estén en una posición similar, eso llevaría a que su representación en el espacio latente fuera muy similar, haciendo corresponder el mismo codemap, y si estos dos rostros se etiquetaban en emociones distintas, entonces es donde empiezan los problemas.\n",
    "\n",
    "En clase se nos mostró un ejemplo con imágenes de aves e incluso se nos mencionó que pasaba que con imágenes de distintas aves, el codemap quedaba muy similar, esto puede ser suficiente como para que, en un problema de clasificación, se logre una precisión tan baja."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
